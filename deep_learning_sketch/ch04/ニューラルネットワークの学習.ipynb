{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ニューラルネットワークの学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 データから学習する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ニューラルネットワークの特徴は「プログラムがデータから自動的に学習する」ということ。  \n",
    "具体的には「プログラムはデータから重みパラメータの値を自動的に更新」して学習を実行する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 損失関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 プログラムは何を指標として、重みパラメータを更新しているのか?\n",
    "\n",
    "ニューラルネットワークの学習は「損失関数」を指標として、損失関数の値が小さくなるように重みパラメータを更新する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 二乗和誤差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "損失関数として、最も有名"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "{ \\displaystyle\n",
    "E = \\frac{1}{2} \\sum_k {(y_k-t_k)}^2}\n",
    "\\tag{4.1} \\label{4.1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ykはニューラルネットワークの出力、tkは教師データを表し、kはデータの次元数を表す。   \n",
    "* ニューラルネットワークの出力と正解となる教師データの各要素の差の2乗の総和が2乗和誤差。   \n",
    "* 出力結果の教師データの誤差が小さい場合，より小さい値を出力する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "二乗和誤差を実装\n",
    "\"\"\"\n",
    "def mean_squared_error(y, t):\n",
    "    return 0.5*np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09750000000000003"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 正解を2とする\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "# パターン1：「2」の割合が最も高い場合\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "\n",
    "mean_squared_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5975"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# パターン2：「7」の割合が最も高い場合\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "\n",
    "mean_squared_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "パターン1の方がパターン2よりも二乗和誤差関数を実装した結果が小さい。  \n",
    "つまり、パターン1の方が出力結果が教師データにより適合していることを二乗和誤差関数が示している。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 交差エントロピー誤差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "{ \\displaystyle\n",
    "E = - \\sum_k t_k \\log y_k}\n",
    "\\tag{4.2} \\label{4.2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tkは2乗和誤差と同様教師データであり、one-hot表現であるため、   \n",
    "(4.2)は実質的には正解ラベルが1に対応する出力の自然対数を計算するだけになっている。   \n",
    "ここで、自然対数のグラフは次のようになっている。   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "交差エントロピー誤差を実装\n",
    "\"\"\"\n",
    "def cross_entropy_error(y, t):\n",
    "     delta = 1e-7    #np.log(0)防止\n",
    "     return -np.sum(t * np.log(y + delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.510825457099338"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 正解を2とする\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "# パターン1：「2」の割合が最も高い場合\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "\n",
    "cross_entropy_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.302584092994546"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# パターン2：「7」の割合が最も高い場合\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "\n",
    "cross_entropy_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正解ラベルに対応する出力の値が大きいほど、交差エントロピー誤差は0に近づいている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 ミニバッチ学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 機械学習の問題は、訓練データを使って学習を行う。   \n",
    "具体的には訓練データに対する損失関数を求め、その値を出来るだけ小さくするようなパラメータを探索するということである。\n",
    "\n",
    "* 損失関数はすべての訓練データを対象として求める必要がある。(訓練データが100個あれば、100個の損失関数の和を求めて、指標とする)\n",
    "\n",
    "* ビックデータの全てに対して、損失関数を求めるのは現実的ではないので、データの中から一部をランダムに選び出す。   \n",
    "これをミニバッチという。そのミニバッチごとに学習を行うのが現実的であり、これをミニバッチ学習と言う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train-images-idx3-ubyte.gz ... \n",
      "Done\n",
      "Downloading train-labels-idx1-ubyte.gz ... \n",
      "Done\n",
      "Downloading t10k-images-idx3-ubyte.gz ... \n",
      "Done\n",
      "Downloading t10k-labels-idx1-ubyte.gz ... \n",
      "Done\n",
      "Converting train-images-idx3-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting train-labels-idx1-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting t10k-images-idx3-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting t10k-labels-idx1-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Creating pickle file ...\n",
      "Done!\n",
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "#MNISTデータの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#訓練データからランダムに10枚抜き出す\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "バッチ対応版 交差エントロピー誤差の実装\n",
    "\"\"\"\n",
    "def cross_entropy_error(y ,t):\n",
    "    if y.dim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    batch_size = y.sahpe[0]\n",
    "    return -np.sum(t * np.log(y)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 数値微分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ニューラルネットワークの学習では、最適なパラメータ(重みとバイアス)を探索する際に、   \n",
    "損失関数の値が出来るだけ小さくなるようにパラメータを探索する。   \n",
    "\n",
    "* 出来るだけ小さな損失関数の値を探すため、パラメータの微分(正確には勾配)を計算している。\n",
    "\n",
    "* 計算された微分の値を手がかりにパラメータの値を徐々に更新している。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 微分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(編集中)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 勾配"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(編集中)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 学習アルゴリズムの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.1 ニューラルネットワークの学習手順"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ステップ1 : 「ミニバッチ」\n",
    "\n",
    "訓練データの中からランダムに一部のデータを選び出す。その選ばれたデータをミニバッチと呼ぶ。   \n",
    "ここでは、そのミニバッチの損失関数を減らすことを目的とする。\n",
    "\n",
    "2. ステップ2 : 「勾配の算出」\n",
    "\n",
    "ミニバッチの損失関数を減らすために、各重みパラメータの勾配を求める。  \n",
    "勾配は、損失関数の値を最も減らす方向を示す。\n",
    "\n",
    "3. ステップ3 : 「パラメータの更新」\n",
    "\n",
    "重みパラメータを勾配方向に微小量だけ更新する。\n",
    "\n",
    "4. ステップ4 : 「繰り返す」\n",
    "\n",
    "ステップ1～ステップ3までを繰り返す。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 まとめ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 機械学習で使用するデータセットは訓練データとテキストデータに分けて使用する。    \n",
    "\n",
    "* 訓練データで学習を行い、学習したモデルの汎化能力をテストデータで評価する。    \n",
    "\n",
    "* ニューラルネットワークの学習は損失関数を指標として、損失関数の値が小さくなるように重みパラメータを更新する。  \n",
    "\n",
    "* 重みパラメータを更新する際には、重みパラメータの勾配を利用して、勾配方向に重みの値を更新する作業を繰り返す。  \n",
    "\n",
    "* 微小な値を与えたときの差分によって微分を求めることを数値微分という。  \n",
    "\n",
    "* 数値微分によって、重みパラメータの勾配を求めることが出来る。  \n",
    "\n",
    "* 数値微分による計算には時間がかかるが、その実装は簡単である。   \n",
    "  一方、次章で実装するやや複雑な誤差逆伝播法は、高速に勾配を求めることが出来る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
