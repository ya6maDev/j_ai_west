{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten, Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras.datasets import cifar10\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "\n",
    "    # extract image features by convolution and max pooling layers\n",
    "    model.add(Conv2D(\n",
    "        32, kernel_size=3, padding=\"same\",\n",
    "        input_shape=input_shape, activation=\"relu\"\n",
    "        ))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(64, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # classify the class by fully-connected layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation=\"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10Dataset():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.image_shape = (32, 32, 3)\n",
    "        self.num_classes = 10\n",
    "\n",
    "    def get_batch(self):\n",
    "        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "        x_train, x_test = [self.preprocess(d) for d in [x_train, x_test]]\n",
    "        y_train, y_test = [self.preprocess(d, label_data=True) for d in\n",
    "                           [y_train, y_test]]\n",
    "\n",
    "        return x_train, y_train, x_test, y_test\n",
    "\n",
    "    def preprocess(self, data, label_data=False):\n",
    "        if label_data:\n",
    "            # convert class vectors to binary class matrices\n",
    "            data = keras.utils.to_categorical(data, self.num_classes)\n",
    "        else:\n",
    "            data = data.astype(\"float32\")\n",
    "            data /= 255  # convert the value to 0~1 scale\n",
    "            shape = (data.shape[0],) + self.image_shape  # add dataset length\n",
    "            data = data.reshape(shape)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "\n",
    "    def __init__(self, model, loss, optimizer):\n",
    "        self._target = model\n",
    "        self._target.compile(\n",
    "            loss=loss, optimizer=optimizer, metrics=[\"accuracy\"]\n",
    "            )\n",
    "        self.verbose = 1\n",
    "        logdir = \"logdir_cifar10_net\"\n",
    "        self.log_dir = os.path.join(os.path.dirname(os.path.abspath(\"__file__\")), logdir)\n",
    "        self.model_file_name = \"model_file.hdf5\"\n",
    "\n",
    "    def train(self, x_train, y_train, batch_size, epochs, validation_split):\n",
    "        if os.path.exists(self.log_dir):\n",
    "            import shutil\n",
    "            shutil.rmtree(self.log_dir)  # remove previous execution\n",
    "        os.mkdir(self.log_dir)\n",
    "\n",
    "        model_path = os.path.join(self.log_dir, self.model_file_name)\n",
    "        self._target.fit(\n",
    "            x_train, y_train,\n",
    "            batch_size=batch_size, epochs=epochs,\n",
    "            validation_split=validation_split,\n",
    "            callbacks=[\n",
    "                TensorBoard(log_dir=self.log_dir),\n",
    "                ModelCheckpoint(model_path, save_best_only=True)\n",
    "            ],\n",
    "            verbose=self.verbose\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CIFAR10Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 512)               2097664   \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 2,122,186\n",
      "Trainable params: 2,122,186\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# make model\n",
    "model = network(dataset.image_shape, dataset.num_classes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "40000/40000 [==============================] - 112s 3ms/step - loss: 1.7261 - acc: 0.3792 - val_loss: 1.4130 - val_acc: 0.5029\n",
      "Epoch 2/12\n",
      "40000/40000 [==============================] - 129s 3ms/step - loss: 1.3023 - acc: 0.5398 - val_loss: 1.1635 - val_acc: 0.5861\n",
      "Epoch 3/12\n",
      "40000/40000 [==============================] - 128s 3ms/step - loss: 1.1404 - acc: 0.5993 - val_loss: 1.0981 - val_acc: 0.6261\n",
      "Epoch 4/12\n",
      "40000/40000 [==============================] - 129s 3ms/step - loss: 1.0324 - acc: 0.6368 - val_loss: 0.9917 - val_acc: 0.6582\n",
      "Epoch 5/12\n",
      "40000/40000 [==============================] - 129s 3ms/step - loss: 0.9560 - acc: 0.6666 - val_loss: 0.9716 - val_acc: 0.6718\n",
      "Epoch 6/12\n",
      "40000/40000 [==============================] - 130s 3ms/step - loss: 0.8921 - acc: 0.6910 - val_loss: 0.8898 - val_acc: 0.6975\n",
      "Epoch 7/12\n",
      "40000/40000 [==============================] - 129s 3ms/step - loss: 0.8328 - acc: 0.7081 - val_loss: 0.8807 - val_acc: 0.6960\n",
      "Epoch 8/12\n",
      "40000/40000 [==============================] - 129s 3ms/step - loss: 0.7797 - acc: 0.7302 - val_loss: 0.8588 - val_acc: 0.7122\n",
      "Epoch 9/12\n",
      "40000/40000 [==============================] - 131s 3ms/step - loss: 0.7320 - acc: 0.7448 - val_loss: 0.8508 - val_acc: 0.7084\n",
      "Epoch 10/12\n",
      "40000/40000 [==============================] - 130s 3ms/step - loss: 0.6889 - acc: 0.7589 - val_loss: 0.8767 - val_acc: 0.7057\n",
      "Epoch 11/12\n",
      "40000/40000 [==============================] - 129s 3ms/step - loss: 0.6555 - acc: 0.7726 - val_loss: 0.8180 - val_acc: 0.7306\n",
      "Epoch 12/12\n",
      "40000/40000 [==============================] - 130s 3ms/step - loss: 0.6258 - acc: 0.7843 - val_loss: 0.8538 - val_acc: 0.7219\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "x_train, y_train, x_test, y_test = dataset.get_batch()\n",
    "trainer = Trainer(model, loss=\"categorical_crossentropy\", optimizer=RMSprop())\n",
    "trainer.train(\n",
    "    x_train, y_train, batch_size=128, epochs=12, validation_split=0.2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.8617481715202332\n",
      "Test accuracy: 0.7146\n"
     ]
    }
   ],
   "source": [
    "# show result\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
